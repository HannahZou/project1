[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bank Marketing",
    "section": "",
    "text": "1 Introduction\nMachine learning is increasingly being integrated across diverse domains to enhance data-driven decision-making. In many cases, complex black-box models are preferred because they are believed to offer better predictive performance. In contrast, interpretable models are frequently overlooked or undervalued, as they are commonly associated with reduced accuracy, despite their potential to offer transparent and trustworthy insights.\nInterpretability helps build user trust, supports regulatory requirements, and makes model results easier to act on. To make complex models more understandable, post-hoc explanation tools like LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive Explanations) are widely used. These methods help explain predictions from black-box models like Random Forests.\nHowever, a major concern is how reliable these explanation methods are. Both LIME and SHAP use random processes—LIME relies on local sampling and perturbations, while SHAP uses Monte Carlo simulations to estimate Shapley values. This means their explanations can vary depending on random seed settings and parameter values (e.g., number of permutations or simulations). For those who are aiming to obtain consistent and reliable model explanations, it becomes essential to evaluate the sensitivity of interpretability methods to variations in factors such as data sampling, model parameters, or random seeds. Such assessments would help to ensure that the insights derived from these explanations remain stable and trustworthy across different conditions.\nThis study investigates how stable the explanations from LIME and SHAP are when explaining a single prediction made by a Random Forest classifier. We examine whether these methods consistently identify the same top features. Furthermore, we compare these locally identified features with the global feature importance rankings derived directly from the Random Forest model, aiming to understand the alignment between local and global interpretability perspectives.\nUsing the Bank Marketing dataset from the UCI Machine Learning Repository, we perform a controlled analysis over multiple runs. We test how different settings affect the results and assess the agreement between local and global explanations. These early findings provide a foundation for larger studies involving more predictions, models, or datasets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html",
    "href": "5293_projcode.html",
    "title": "3  5293 Project",
    "section": "",
    "text": "3.1 Load and Preprocess Data\nCode\n# Load the dataset\nbank &lt;- fetch_ucirepo(\"Bank Marketing\")\ndf &lt;- bind_cols(bank$data$features, y = bank$data$targets$y)\n\n# Encode categorical features as factors\ndf[] &lt;- lapply(df, function(col) if (is.character(col) || is.logical(col)) as.factor(col) else col)\n\n# Remove high-cardinality categorical variables\nvalid_cols &lt;- sapply(df, function(col) !is.factor(col) || nlevels(col) &lt;= 20)\ndf &lt;- df[, valid_cols]\n\n# Ensure target variable is a factor\ndf$y &lt;- as.factor(df$y)\nThe dataset includes 45,211 observations and 17 predictor variables, encompassing both categorical and numeric features such as:\nThe target variable, y, is binary and indicates whether the client subscribed to a term deposit (yes or no). First, we imported the dataset by ucimlrepo R package. Then, all categorical variables were converted to factors for compatibility with the randomForest, lime, and Fastshap packages. And the high-cardinality categorical features (with more than 20 unique levels) were removed to simplify analysis and reduce sparsity in local surrogate models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#load-and-preprocess-data",
    "href": "5293_projcode.html#load-and-preprocess-data",
    "title": "3  5293 Project",
    "section": "",
    "text": "Client Attributes: age, job, marital, education\nBanking Information: default, housing, loan\nContact Details: contact, month, day_of_week, duration\nCampaign Metrics: campaign, pdays, previous, poutcome",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#traintest-split",
    "href": "5293_projcode.html#traintest-split",
    "title": "3  5293 Project",
    "section": "3.2 Train/Test Split",
    "text": "3.2 Train/Test Split\n\n\nCode\nset.seed(5293)\nsplit_idx &lt;- createDataPartition(df$y, p = 0.8, list = FALSE)\ntrain_data &lt;- df[split_idx, ]\ntest_data &lt;- df[-split_idx, ]\n\ntrain_X &lt;- train_data %&gt;% select(-y)\ntrain_y &lt;- train_data$y\ntest_X &lt;- test_data %&gt;% select(-y)\ntest_y &lt;- test_data$y\n\n\nNext, a stratified 80/20 train-test split was applied using caret::createDataPartition() to ensure balanced representation of the target classes in both sets. The final working dataset contains a mix of numerical and low-cardinality categorical features, enabling a robust evaluation of local explanation techniques on structured data typical of banking and marketing domains.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#train-random-forest-evaluate",
    "href": "5293_projcode.html#train-random-forest-evaluate",
    "title": "3  5293 Project",
    "section": "3.3 Train Random Forest & Evaluate",
    "text": "3.3 Train Random Forest & Evaluate\n\n\nCode\n# Train RF\nset.seed(5293)\nrf_model &lt;- randomForest(y ~ ., data = train_data, ntree = 100, importance = TRUE)\n\n# Predict\ntrain_preds &lt;- predict(rf_model, newdata = train_data)\ntest_preds &lt;- predict(rf_model, newdata = test_data)\n\n# Accuracy\ntrain_acc &lt;- mean(train_preds == train_data$y)\ntest_acc &lt;- mean(test_preds == test_data$y)\n\n# Precision (positive class: \"yes\")\nprecision &lt;- function(true, pred) {\n  cm &lt;- confusionMatrix(pred, true, positive = \"yes\")\n  cm$byClass[\"Precision\"]\n}\n\ncat(\"Training Accuracy:\", round(train_acc, 3), \"\\n\")\n\n\nTraining Accuracy: 1 \n\n\nCode\ncat(\"Test Accuracy:\", round(test_acc, 3), \"\\n\")\n\n\nTest Accuracy: 0.908 \n\n\nCode\ncat(\"Test Precision:\", round(precision(test_data$y, test_preds), 3), \"\\n\")\n\n\nTest Precision: 0.637 \n\n\nCode\n# Generate confusion matrix for test data\ncm &lt;- confusionMatrix(test_preds, test_data$y, positive = \"yes\")\n\n# Extract recall and F1\nrecall &lt;- cm$byClass[\"Recall\"]\nf1 &lt;- cm$byClass[\"F1\"]\n\n# Print\ncat(\"Recall:\", round(recall, 3), \"\\n\")\n\n\nRecall: 0.491 \n\n\nCode\ncat(\"F1 Score:\", round(f1, 3), \"\\n\")\n\n\nF1 Score: 0.554 \n\n\nCode\ncat(\"Confusion Matrix:\\n\")\n\n\nConfusion Matrix:\n\n\nCode\nprint(cm$table)\n\n\n          Reference\nPrediction   no  yes\n       no  7688  538\n       yes  296  519\n\n\nA Random Forest model comprising 100 trees (ntree = 100) was trained on the preprocessed training dataset to serve as the predictive model for subsequent interpretation. Performance was assessed on both the training and test sets using confusion matrix metrics, including accuracy and precision for the positive class. Global feature importance was derived via the Mean Decrease in Gini Impurity and visualized using the varImpPlot() and vip() functions.\n\nTraining Accuracy is 1, means the model made no mistakes on the training data, which indicates that perfect fit on the training set, but it’s a red flag for potential overfitting—the model may be memorizing rather than generalizing patterns. 2. Test Accuracy is 0.908, suggesting that nearly 90.8% of predictions on the test set were correct. Based on the output, we consider it as a reasonably high test accuracy, suggesting the model generalizes well, though not perfectly.However, the accuracy onlt can be misleading if the data is imbalanced (e.g., far more “no” than “yes” cases).3. Test Precision (for “yes”) is 0.637 shows of all the cases the model predicted as “yes”, only 63.7% were actually “yes”. This shows the model has a moderate rate of false positives—it sometimes predicts “yes” when the true label is “no”. 4. Recall (0.491) is low, meaning the model misses over half of the real “yes” cases. and F1 Score (0.554) indicates a moderate balance, but not ideal for critical tasks where recall is important.\n\nGenerally, in our case of bank marketing, losing a truly interested customer (FN) means missing a potential gain, which outweighs the relatively lower cost of marketing to an uninterested one (FP) –&gt; false negatives are generally more harmful than false positives, this model would be acceptable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#variable-importance-plot",
    "href": "5293_projcode.html#variable-importance-plot",
    "title": "3  5293 Project",
    "section": "3.4 Variable Importance Plot",
    "text": "3.4 Variable Importance Plot\n\n\nCode\n# Global feature importance\nvarImpPlot(rf_model, main = \"Variable Importance (Random Forest)\")\n\n\n\n\n\n\n\n\n\nThe vaiable imporatnce plot shows which features most influenced the random forest’s decisions.\nFor MeanDecreaseAccuracy plot, it measures how much model accuracy decreases when each variable is randomly permuted. For example, duriation causing a big drop in accuracy from 120 to 50, which indicates that feature (duration) is more important. Next is month, day_of_week,age, contact.\nFor the MeanDecreaseGini plot, it measures how much each feature reduces node impurity across all trees (i.e., Gini impurity).The Higher values such as duriation, month, balance, age, and day_of_week mean the feature often contributes to good splits. This is an internal Random Forest metric—not directly tied to accuracy, but to the quality of splits.\n\n\nCode\nvip::vip(rf_model, num_features = 10, geom = \"col\") + ggtitle(\"Random Forest Global Feature Importance\")\n\n\n\n\n\n\n\n\n\nFrom the random Forest Global feature importance generated by the VIP package, the top influential features is duration (Call Duration in Seconds). we infer that the longer the call, the more likely the client said “yes”. This makes intuitive sense—longer conversations often signal engagement or interest. However, duration is not known before the call. It’s not usable for real-time prediction, though still useful for retrospective modeling or explanation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#pdp-for-top-features",
    "href": "5293_projcode.html#pdp-for-top-features",
    "title": "3  5293 Project",
    "section": "3.5 PDP for Top Features",
    "text": "3.5 PDP for Top Features\n\n\nCode\n# Top 3 important features from RF\ntop_vars &lt;- names(sort(rf_model$importance[, \"MeanDecreaseGini\"], decreasing = TRUE))[1:3]\n\n# PDPs for top 3 features using autoplot (which returns ggplot object)\nfor (var in top_vars) {\n  pd &lt;- pdp::partial(rf_model, pred.var = var, train = train_X, prob = TRUE)\n  print(autoplot(pd) + ggtitle(paste(\"PDP -\", var)))\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#faceted-pdps-for-numeric-features-only",
    "href": "5293_projcode.html#faceted-pdps-for-numeric-features-only",
    "title": "3  5293 Project",
    "section": "3.6 Faceted PDPs for Numeric Features Only",
    "text": "3.6 Faceted PDPs for Numeric Features Only\n\n\nCode\n# Extract numeric features\nnumeric_vars &lt;- names(train_X)[sapply(train_X, is.numeric)]\n\n# Collect PDPs in long format\npdp_long_list &lt;- lapply(numeric_vars, function(var) {\n  pd &lt;- pdp::partial(rf_model, pred.var = var, train = train_X, prob = TRUE)\n  colnames(pd)[1] &lt;- \"x\"  # rename feature column to \"x\"\n  pd$feature &lt;- var       # store original feature name for faceting\n  pd\n})\n\n# Combine all PDPs into one data frame\npdp_all &lt;- bind_rows(pdp_long_list)\n\n# Faceted PDP plot\nggplot(pdp_all, aes(x = x, y = yhat)) +\n  geom_line() +\n  facet_wrap(~feature, scales = \"free_x\") +\n  labs(\n    title = \"Faceted PDPs for Numeric Features\",\n    x = \"Feature Value\",\n    y = \"Predicted Probability (yes)\"\n  )\n\n\n\n\n\n\n\n\n\nThen, we faceted PDP (Partial Dependence Plot) to visualize the numerical featrues, to show the effect of those features on the predicted probaility of subscibing (y is “yes”) in the Bank Marketing dataset. From the plot, we observe that some features, such as duration and age, exhibit strong non-linear relationships with the prediction outcome, indicating they play a great role in the model’s decision-making. In contrast, other variables such as campaign, previous, and pdays show relatively flat curves, suggesting limited standalone predictive power.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#shap-explanation-fastshap",
    "href": "5293_projcode.html#shap-explanation-fastshap",
    "title": "3  5293 Project",
    "section": "3.7 SHAP Explanation (fastshap)",
    "text": "3.7 SHAP Explanation (fastshap)\n\n\nCode\n# SHAP prediction function\npfun_rf &lt;- function(object, newdata) {\n  predict(object, newdata, type = \"prob\")[, \"yes\"]\n}\n\n# Explain row 3\nset.seed(2024)\nshap_values &lt;- fastshap::explain(\n  object = rf_model,\n  X = train_X,\n  newdata = test_X[3, , drop = FALSE],\n  pred_wrapper = pfun_rf,\n  nsim = 100,\n  adjust = TRUE\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#shap-plot",
    "href": "5293_projcode.html#shap-plot",
    "title": "3  5293 Project",
    "section": "3.8 SHAP Plot",
    "text": "3.8 SHAP Plot\n\n\nCode\n# Convert SHAP values to data frame for plotting\nshap_df &lt;- shap_values %&gt;%\n  as.data.frame() %&gt;%\n  t() %&gt;%\n  as.data.frame() %&gt;%\n  tibble::rownames_to_column(\"feature\") %&gt;%\n  rename(shap_value = V1) %&gt;%\n  arrange(desc(abs(shap_value)))\n\n# Plot top 5 SHAP features\nggplot(shap_df[1:5, ], aes(x = reorder(feature, shap_value), y = shap_value)) +\n  geom_col() +\n  coord_flip() +\n  ggtitle(\"SHAP Explanation (Row 3)\") +\n  xlab(\"Feature\") +\n  ylab(\"SHAP value\")\n\n\n\n\n\n\n\n\n\nAll features on the graph shows a negative SHAP value contributed to the final prediction, indicates that they decreased the probability that the model predicted “yes” for subscription. For example, for one local sample (row 3), the duration illustrated the Largest negative SHAP value (≈ -0.045). It suggests that this customer’s call duration was short, which correlates with a lower likelihood of subscription. And the contact (The method of contact (e.g., “telephone”)), is negatively influenced the prediction. we infer that the cellular contact performs better than telephone.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#lime-explanation",
    "href": "5293_projcode.html#lime-explanation",
    "title": "3  5293 Project",
    "section": "3.9 LIME Explanation",
    "text": "3.9 LIME Explanation\n\n\nCode\nlibrary(lime)\n# Tell lime how to handle randomForest models\nmodel_type.randomForest &lt;- function(x, ...) {\n  return(\"classification\")\n}\n\npredict_model.randomForest &lt;- function(x, newdata, type, ...) {\n  data.frame(predict(x, newdata = newdata, type = \"prob\"))\n}\n\n# Ensure input is a data.frame\ntrain_X_df &lt;- as.data.frame(train_X)\ntest_X_df &lt;- as.data.frame(test_X)\n\n# Create explainer\nexplainer_lime &lt;- lime(train_X_df, rf_model)\n\n# Explain one test instance\nset.seed(123)\nlime_result &lt;- lime::explain(\n  x = test_X_df[3, , drop = FALSE],\n  explainer = explainer_lime,\n  n_labels = 1,\n  n_features = 5,\n  n_permutations = 1000\n)\n\n# Plot explanation\nplot_features(lime_result) + ggtitle(\"LIME Explanation (Row 3)\")\n\n\n\n\n\n\n\n\n\nNext, we apply the LIME explaination for the also for the smae observation (row 3). It explains the prediction for Row 3, where the model predicted “no” (not subscribing) with 100% confidence (Probability: 1). The LIME explanation fit is low (0.14), indicating the linear approximation fits the local decision surface weakly but still provides useful directional insight. For Features supporting the prediction (blue bars), the weights of duriation is between 103 and 179, suggesting that the short call duration is the strongest supporter of the “no” prediction. Similar to the Month in May, we infer this month likely corresponds with low subscription rates. For Features contradicting the prediction (red bars), if the customer is older than 48, which typically correlates with higher probabilities of subscription, but here it pushes against the “no” label. Besides, fewer contacts might have historically resulted in success, so this feature also contradicts the “no” prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#compare-top-features-lime-vs-shap-vs-rf",
    "href": "5293_projcode.html#compare-top-features-lime-vs-shap-vs-rf",
    "title": "3  5293 Project",
    "section": "3.10 Compare Top Features (LIME vs SHAP vs RF)",
    "text": "3.10 Compare Top Features (LIME vs SHAP vs RF)\n\n\nCode\n# Rebuild LIME explainer if needed\nexplainer_lime &lt;- lime(train_X, rf_model)\n\n# Run LIME explanation again\nset.seed(123)\nlime_result &lt;- lime::explain(\n  x = test_X[3, , drop = FALSE],\n  explainer = explainer_lime,\n  n_labels = 1,\n  n_features = 5,\n  n_permutations = 1000\n)\n\n# Top features from each method\ntop_lime &lt;- lime_result %&gt;%\n  arrange(desc(abs(feature_weight))) %&gt;%\n  pull(feature) %&gt;%\n  unique() %&gt;%\n  head(5)\n\ntop_shap &lt;- shap_df %&gt;%\n  pull(feature) %&gt;%\n  head(5)\n\ntop_rf &lt;- randomForest::importance(rf_model) %&gt;%\n  as.data.frame() %&gt;%\n  tibble::rownames_to_column(\"feature\") %&gt;%\n  arrange(desc(MeanDecreaseGini)) %&gt;%\n  pull(feature) %&gt;%\n  head(5)\n\n# Jaccard similarity function\njaccard &lt;- function(x, y) length(intersect(x, y)) / length(union(x, y))\n\n# Print similarities\ncat(\"Jaccard Similarity (LIME vs SHAP):\", jaccard(top_lime, top_shap), \"\\n\")\n\n\nJaccard Similarity (LIME vs SHAP): 0.25 \n\n\nCode\ncat(\"Jaccard Similarity (LIME vs RF):\", jaccard(top_lime, top_rf), \"\\n\")\n\n\nJaccard Similarity (LIME vs RF): 0.6666667 \n\n\nCode\ncat(\"Jaccard Similarity (SHAP vs RF):\", jaccard(top_shap, top_rf), \"\\n\")\n\n\nJaccard Similarity (SHAP vs RF): 0.25 \n\n\nAnd we use three different seed and also try n_permutation for 1000 times in SHAP, For the agreement Between Explanation Methods, the LIME vs SHAP similarity was low (0.25), indicating significant divergence in how these methods attributed importance for the selected instance. In contrast, LIME showed stronger alignment with the global Random Forest feature importance (Jaccard = 0.67), suggesting that LIME’s local explanation partially reflects the model’s global behavior. SHAP, on the other hand, exhibited lower agreement with the Random Forest (Jaccard = 0.25), potentially highlighting context-specific influences that are not reflected in global importance metrics. Additionally, two features (pdays and previous) triggered a fallback to standard binning due to low variance. This may affect explanation granularity or mask subtle differences in feature impact.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#lime-stability-over-seeds",
    "href": "5293_projcode.html#lime-stability-over-seeds",
    "title": "3  5293 Project",
    "section": "3.11 LIME Stability Over Seeds",
    "text": "3.11 LIME Stability Over Seeds\n\n\nCode\nlibrary(lime)\n# Function to extract top 5 LIME features with different seeds\nget_lime_top_features &lt;- function(seed_val) {\n  set.seed(seed_val)\n  result &lt;- lime::explain(\n    x = test_X[5, , drop = FALSE],\n    explainer = explainer_lime,\n    n_labels = 1,\n    n_features = 5,\n    n_permutations = 1000\n  )\n  result %&gt;%\n    arrange(desc(abs(feature_weight))) %&gt;%\n    pull(feature) %&gt;%\n    head(5)\n}\n\n# Run for 3 seeds\nlime_seed1 &lt;- get_lime_top_features(1)\nlime_seed2 &lt;- get_lime_top_features(2)\nlime_seed3 &lt;- get_lime_top_features(3)\nlime_seed1\n\n\n[1] \"duration\" \"age\"      \"month\"    \"campaign\" \"housing\" \n\n\nCode\nlime_seed2\n\n\n[1] \"duration\" \"age\"      \"month\"    \"housing\"  \"loan\"    \n\n\nCode\nlime_seed3\n\n\n[1] \"duration\"    \"age\"         \"month\"       \"day_of_week\" \"balance\"    \n\n\nCode\n# Print pairwise stability\ncat(\"LIME (Seed 1 vs Seed 2):\", jaccard(lime_seed1, lime_seed2), \"\\n\")\n\n\nLIME (Seed 1 vs Seed 2): 0.6666667 \n\n\nCode\ncat(\"LIME (Seed 1 vs Seed 3):\", jaccard(lime_seed1, lime_seed3), \"\\n\")\n\n\nLIME (Seed 1 vs Seed 3): 0.4285714 \n\n\nCode\ncat(\"LIME (Seed 2 vs Seed 3):\", jaccard(lime_seed2, lime_seed3), \"\\n\")\n\n\nLIME (Seed 2 vs Seed 3): 0.4285714 \n\n\nAnd we use three different seed and also try n_permutation for 1000 times in LIME, The stability results show that LIME explanations are moderately sensitive to random seed variation. The features “duration”, “age”, and “month” appeared in all three runs, indicating that these attributes consistently influence the model’s prediction and can be considered robust drivers of local interpretability. However, the inclusion of other features (e.g., “campaign”, “loan”, “day_of_week”, and “balance”) varied depending on the seed. This variability suggests that LIME’s perturbation-based approximation introduces non-negligible randomness into the explanation process, which can affect feature selection in local interpretations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#visual-inspection-comparison-of-feature-attribution-plots-for-interpretability",
    "href": "5293_projcode.html#visual-inspection-comparison-of-feature-attribution-plots-for-interpretability",
    "title": "3  5293 Project",
    "section": "3.12 Visual Inspection: Comparison of feature attribution plots for interpretability",
    "text": "3.12 Visual Inspection: Comparison of feature attribution plots for interpretability\n\n\nCode\n# Combine top-5 features from LIME, SHAP, and Random Forest\nlibrary(tidyr)\n\n# Create data frames with ranks for each method\ndf_lime &lt;- data.frame(feature = top_lime, rank = 1:5, method = \"LIME\")\ndf_shap &lt;- data.frame(feature = top_shap, rank = 1:5, method = \"SHAP\")\ndf_rf &lt;- data.frame(feature = top_rf, rank = 1:5, method = \"Random Forest\")\n\n# Combine into one table\ndf_all &lt;- bind_rows(df_lime, df_shap, df_rf)\n\n# Plot: Bar chart comparing feature ranks from different methods\nggplot(df_all, aes(x = reorder(feature, -rank), y = 6 - rank, fill = method)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Comparison of Top 5 Feature Rankings\",\n    x = \"Feature\",\n    y = \"Rank (Higher is More Important)\"\n  ) +\n  scale_y_continuous(breaks = 1:5, labels = rev(1:5)) +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#lime-stability-over-seeds-and-n_permutations",
    "href": "5293_projcode.html#lime-stability-over-seeds-and-n_permutations",
    "title": "3  5293 Project",
    "section": "3.13 LIME Stability Over Seeds and n_permutations",
    "text": "3.13 LIME Stability Over Seeds and n_permutations\n\n\nCode\n# Define parameters\nn_permutations_list &lt;- c(10, 50, 100, 1000)\nseed_list &lt;- c(1, 2, 3)\n\n# Collect results\nlime_results &lt;- list()\n\n# Loop over permutations and seeds\nfor (n_perm in n_permutations_list) {\n  for (s in seed_list) {\n    set.seed(s)\n    explanation &lt;- lime::explain(\n      x = test_X[3, , drop = FALSE],\n      explainer = explainer_lime,\n      n_labels = 1,\n      n_features = 5,\n      n_permutations = n_perm\n    )\n    \n    top_features &lt;- explanation %&gt;%\n      arrange(desc(abs(feature_weight))) %&gt;%\n      pull(feature) %&gt;%\n      unique() %&gt;%\n      head(5)\n    \n    label &lt;- paste0(\"perm_\", n_perm, \"_seed_\", s)\n    lime_results[[label]] &lt;- top_features\n  }\n}\n\n\n\n\nCode\n# Flatten to frequency table\nfeature_counts &lt;- unlist(lime_results) %&gt;%\n  table() %&gt;%\n  sort(decreasing = TRUE) %&gt;%\n  as.data.frame()\ncolnames(feature_counts) &lt;- c(\"feature\", \"count\")\n\n# Plot frequency of appearance\nggplot(feature_counts, aes(x = reorder(feature, count), y = count)) +\n  geom_col(fill = \"skyblue\") +\n  coord_flip() +\n  labs(\n    title = \"Frequency of Feature Appearance in Top-5 (LIME)\",\n    x = \"Feature\",\n    y = \"Count (out of 12)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis bar chart displays how frequently each feature appeared in the top 5 most important features identified by LIME across 12 runs — derived from 4 different n_permutations values (10, 50, 100, 1000) and 3 different random seeds per setting (4 × 3 = 12 total combinations). and we find that the duration appeared in all 12 runs, making it the most consistently important feature. Next, month and age followed closely, appearing in 9 and 8 runs, respectively. For Features like campaign, housing, and loan appeared in roughly 5–6 runs, suggesting moderate importance but some sensitivity to LIME parameters.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  },
  {
    "objectID": "5293_projcode.html#shap-stability-over-seeds-and-n_permutations",
    "href": "5293_projcode.html#shap-stability-over-seeds-and-n_permutations",
    "title": "3  5293 Project",
    "section": "3.14 SHAP Stability Over Seeds and n_permutations",
    "text": "3.14 SHAP Stability Over Seeds and n_permutations\n\n\nCode\n# Define settings\nshap_nsim_list &lt;- c(10, 50, 100, 1000)\nshap_seed_list &lt;- c(1, 2, 3)\n\n# Prediction wrapper for SHAP\npfun_rf &lt;- function(object, newdata) {\n  predict(object, newdata, type = \"prob\")[, \"yes\"]\n}\n\n# Store top-5 features per SHAP explanation\nshap_results &lt;- list()\n\nfor (nsim in shap_nsim_list) {\n  for (s in shap_seed_list) {\n    set.seed(s)\n    shap_vals &lt;- fastshap::explain(\n      object = rf_model,\n      X = train_X,\n      newdata = test_X[3, , drop = FALSE],\n      pred_wrapper = pfun_rf,\n      nsim = nsim,\n      adjust = TRUE\n    )\n    \n    shap_df &lt;- shap_vals %&gt;%\n      as.data.frame() %&gt;%\n      t() %&gt;%\n      as.data.frame() %&gt;%\n      tibble::rownames_to_column(\"feature\") %&gt;%\n      rename(shap_value = V1) %&gt;%\n      arrange(desc(abs(shap_value)))\n    \n    label &lt;- paste0(\"nsim_\", nsim, \"_seed_\", s)\n    shap_results[[label]] &lt;- head(shap_df$feature, 5)\n  }\n}\nshap_results\n\n\n$nsim_10_seed_1\n[1] \"month\"       \"housing\"     \"day_of_week\" \"duration\"    \"campaign\"   \n\n$nsim_10_seed_2\n[1] \"previous\" \"pdays\"    \"poutcome\" \"housing\"  \"contact\" \n\n$nsim_10_seed_3\n[1] \"month\"    \"duration\" \"housing\"  \"previous\" \"marital\" \n\n$nsim_50_seed_1\n[1] \"duration\" \"month\"    \"previous\" \"housing\"  \"marital\" \n\n$nsim_50_seed_2\n[1] \"duration\" \"month\"    \"pdays\"    \"previous\" \"campaign\"\n\n$nsim_50_seed_3\n[1] \"month\"    \"duration\" \"previous\" \"balance\"  \"housing\" \n\n$nsim_100_seed_1\n[1] \"duration\" \"month\"    \"contact\"  \"previous\" \"housing\" \n\n$nsim_100_seed_2\n[1] \"month\"    \"duration\" \"previous\" \"age\"      \"job\"     \n\n$nsim_100_seed_3\n[1] \"month\"    \"poutcome\" \"duration\" \"previous\" \"balance\" \n\n$nsim_1000_seed_1\n[1] \"month\"    \"duration\" \"previous\" \"housing\"  \"pdays\"   \n\n$nsim_1000_seed_2\n[1] \"duration\" \"month\"    \"previous\" \"housing\"  \"pdays\"   \n\n$nsim_1000_seed_3\n[1] \"duration\" \"month\"    \"previous\" \"pdays\"    \"housing\" \n\n\n\n\nCode\n# Count how often each feature appears\nshap_feature_counts &lt;- unlist(shap_results) %&gt;%\n  table() %&gt;%\n  sort(decreasing = TRUE) %&gt;%\n  as.data.frame()\ncolnames(shap_feature_counts) &lt;- c(\"feature\", \"count\")\n\nggplot(shap_feature_counts, aes(x = reorder(feature, count), y = count)) +\n  geom_col(fill = \"blue\") +\n  coord_flip() +\n  labs(\n    title = \"Frequency of Feature Appearance in Top-5 (SHAP)\",\n    x = \"Feature\",\n    y = \"Count (out of 12)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe SHAP frequency analysis mentioned a high degree of stability for core features like previous, month, and duration. These features probablely capture global patterns that SHAP preserve across different runs due to its model-faithful nature. In contrast, features like job, age, and day_of_week appear rarely, suggesting they are less influential or context-dependent for the instance being explained.\nTo evaluate the robustness of local feature attributions, we compared LIME and SHAP across 12 explanation runs for a single test instance. Each method was evaluated under four levels of n_permutations (10, 50, 100, 1000) with three different seeds per setting. We recorded how frequently each feature appeared in the top 5 most important features. The results indicate that SHAP provided greater stability, with features like previous, month, and duration consistently appearing in all 12 runs. This suggests that SHAP’s attribution is more robust and less sensitive to randomness in the explanation process. In contrast, LIME exhibited more variability, especially for secondary features. Although duration was consistently the most important feature, the inclusion of other features such as campaign, loan, and job fluctuated across runs.Notably, both methods consistently highlighted duration and month, suggesting that these features are genuinely influential for the model’s decision on this instance. However, the divergence in secondary feature rankings illustrates the importance of conducting stability analyses when using post-hoc interpretability methods, especially in high-stakes applications.\nGenerally, the findings underscore the trade-off between LIME’s intuitive local surrogate modeling and SHAP’s more theoretically grounded and consistent attributions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>5293 Project</span>"
    ]
  }
]