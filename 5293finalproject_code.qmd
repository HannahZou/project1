---
title: "stat5293_final_project_yz4821"
format: html
editor: visual
---

```{r}
# Load libraries
library(ucimlrepo)
library(dplyr)
library(caret)
library(randomForest)
library(lime)
library(fastshap)
library(ggplot2)
library(Matrix)
library(vip)

```

# 1. Load and Preprocess Data

```{r}
# Load the dataset
bank <- fetch_ucirepo("Bank Marketing")
df <- bind_cols(bank$data$features, y = bank$data$targets$y)

# Encode categorical features as factors
df[] <- lapply(df, function(col) if (is.character(col) || is.logical(col)) as.factor(col) else col)

# Remove high-cardinality categorical variables
valid_cols <- sapply(df, function(col) !is.factor(col) || nlevels(col) <= 20)
df <- df[, valid_cols]

# Ensure target variable is a factor
df$y <- as.factor(df$y)

```

# 2. Train/Test Split

```{r}
set.seed(5293)
split_idx <- createDataPartition(df$y, p = 0.8, list = FALSE)
train_data <- df[split_idx, ]
test_data <- df[-split_idx, ]

train_X <- train_data %>% select(-y)
train_y <- train_data$y
test_X <- test_data %>% select(-y)
test_y <- test_data$y

```

# 3. Train Random Forest & Evaluate

```{r}
# Train RF
set.seed(5293)
rf_model <- randomForest(y ~ ., data = train_data, ntree = 100, importance = TRUE)

# Predict
train_preds <- predict(rf_model, newdata = train_data)
test_preds <- predict(rf_model, newdata = test_data)

# Accuracy
train_acc <- mean(train_preds == train_data$y)
test_acc <- mean(test_preds == test_data$y)

# Precision (positive class: "yes")
precision <- function(true, pred) {
  cm <- confusionMatrix(pred, true, positive = "yes")
  cm$byClass["Precision"]
}

cat("Training Accuracy:", round(train_acc, 3), "\n")
cat("Test Accuracy:", round(test_acc, 3), "\n")
cat("Test Precision:", round(precision(test_data$y, test_preds), 3), "\n")

```

# 4. Variable Importance Plot

```{r}
# Global feature importance
varImpPlot(rf_model, main = "Variable Importance (Random Forest)")
```

```{r}
vip::vip(rf_model, num_features = 10, geom = "col") + ggtitle("Random Forest Global Feature Importance")
```

# 5. PDP for Top Features

```{r}
# Top 3 important features from RF
top_vars <- names(sort(rf_model$importance[, "MeanDecreaseGini"], decreasing = TRUE))[1:3]

# PDPs for top 3 features using autoplot (which returns ggplot object)
for (var in top_vars) {
  pd <- pdp::partial(rf_model, pred.var = var, train = train_X, prob = TRUE)
  print(autoplot(pd) + ggtitle(paste("PDP -", var)))
}

```

# 6. Faceted PDPs for Numeric Features Only

```{r}
# Extract numeric features
numeric_vars <- names(train_X)[sapply(train_X, is.numeric)]

# Collect PDPs in long format
pdp_long_list <- lapply(numeric_vars, function(var) {
  pd <- pdp::partial(rf_model, pred.var = var, train = train_X, prob = TRUE)
  colnames(pd)[1] <- "x"  # rename feature column to "x"
  pd$feature <- var       # store original feature name for faceting
  pd
})

# Combine all PDPs into one data frame
pdp_all <- bind_rows(pdp_long_list)

# Faceted PDP plot
ggplot(pdp_all, aes(x = x, y = yhat)) +
  geom_line() +
  facet_wrap(~feature, scales = "free_x") +
  labs(
    title = "Faceted PDPs for Numeric Features",
    x = "Feature Value",
    y = "Predicted Probability (yes)"
  )

```

# 7. SHAP Explanation (fastshap)

```{r}
# SHAP prediction function
pfun_rf <- function(object, newdata) {
  predict(object, newdata, type = "prob")[, "yes"]
}

# Explain row 3
set.seed(2024)
shap_values <- fastshap::explain(
  object = rf_model,
  X = train_X,
  newdata = test_X[3, , drop = FALSE],
  pred_wrapper = pfun_rf,
  nsim = 100,
  adjust = TRUE
)

```

# 8. SHAP Plot

```{r}
# Convert SHAP values to data frame for plotting
shap_df <- shap_values %>%
  as.data.frame() %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column("feature") %>%
  rename(shap_value = V1) %>%
  arrange(desc(abs(shap_value)))

# Plot top 5 SHAP features
ggplot(shap_df[1:5, ], aes(x = reorder(feature, shap_value), y = shap_value)) +
  geom_col() +
  coord_flip() +
  ggtitle("SHAP Explanation (Row 3)") +
  xlab("Feature") +
  ylab("SHAP value")

```

# 9. LIME Explanation

```{r}
explainer_lime <- lime(train_X, rf_model)

set.seed(123)
lime_result <- lime::explain(
  x = test_X[3, , drop = FALSE],
  explainer = explainer_lime,
  n_labels = 1,
  n_features = 5,
  n_permutations = 1000
)

plot_features(lime_result) + ggtitle("LIME Explanation (Row 3)")

```

# 10. Compare Top Features (LIME vs SHAP vs RF)

```{r}
# Rebuild LIME explainer if needed
explainer_lime <- lime(train_X, rf_model)

# Run LIME explanation again
set.seed(123)
lime_result <- lime::explain(
  x = test_X[3, , drop = FALSE],
  explainer = explainer_lime,
  n_labels = 1,
  n_features = 5,
  n_permutations = 1000
)

# Top features from each method
top_lime <- lime_result %>%
  arrange(desc(abs(feature_weight))) %>%
  pull(feature) %>%
  unique() %>%
  head(5)

top_shap <- shap_df %>%
  pull(feature) %>%
  head(5)

top_rf <- randomForest::importance(rf_model) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("feature") %>%
  arrange(desc(MeanDecreaseGini)) %>%
  pull(feature) %>%
  head(5)

# Jaccard similarity function
jaccard <- function(x, y) length(intersect(x, y)) / length(union(x, y))

# Print similarities
cat("Jaccard Similarity (LIME vs SHAP):", jaccard(top_lime, top_shap), "\n")
cat("Jaccard Similarity (LIME vs RF):", jaccard(top_lime, top_rf), "\n")
cat("Jaccard Similarity (SHAP vs RF):", jaccard(top_shap, top_rf), "\n")

```

# 11. LIME Stability Over Seeds

```{r}
# Function to extract top 5 LIME features with different seeds
get_lime_top_features <- function(seed_val) {
  set.seed(seed_val)
  result <- lime::explain(
    x = test_X[5, , drop = FALSE],
    explainer = explainer_lime,
    n_labels = 1,
    n_features = 5,
    n_permutations = 1000
  )
  result %>%
    arrange(desc(abs(feature_weight))) %>%
    pull(feature) %>%
    head(5)
}

# Run for 3 seeds
lime_seed1 <- get_lime_top_features(1)
lime_seed2 <- get_lime_top_features(2)
lime_seed3 <- get_lime_top_features(3)
lime_seed1
lime_seed2
lime_seed3
# Print pairwise stability
cat("LIME (Seed 1 vs Seed 2):", jaccard(lime_seed1, lime_seed2), "\n")
cat("LIME (Seed 1 vs Seed 3):", jaccard(lime_seed1, lime_seed3), "\n")
cat("LIME (Seed 2 vs Seed 3):", jaccard(lime_seed2, lime_seed3), "\n")

```

# 12. Visual Inspection: Comparison of feature attribution plots for interpretability

```{r}
# Combine top-5 features from LIME, SHAP, and Random Forest
library(tidyr)

# Create data frames with ranks for each method
df_lime <- data.frame(feature = top_lime, rank = 1:5, method = "LIME")
df_shap <- data.frame(feature = top_shap, rank = 1:5, method = "SHAP")
df_rf <- data.frame(feature = top_rf, rank = 1:5, method = "Random Forest")

# Combine into one table
df_all <- bind_rows(df_lime, df_shap, df_rf)

# Plot: Bar chart comparing feature ranks from different methods
ggplot(df_all, aes(x = reorder(feature, -rank), y = 6 - rank, fill = method)) +
  geom_col(position = "dodge") +
  labs(
    title = "Comparison of Top 5 Feature Rankings",
    x = "Feature",
    y = "Rank (Higher is More Important)"
  ) +
  scale_y_continuous(breaks = 1:5, labels = rev(1:5)) +
  theme_minimal()

```

# 13. LIME Stability Over Seeds and n_permutations

```{r}
# Define parameters
n_permutations_list <- c(10, 50, 100, 1000)
seed_list <- c(1, 2, 3)

# Collect results
lime_results <- list()

# Loop over permutations and seeds
for (n_perm in n_permutations_list) {
  for (s in seed_list) {
    set.seed(s)
    explanation <- lime::explain(
      x = test_X[3, , drop = FALSE],
      explainer = explainer_lime,
      n_labels = 1,
      n_features = 5,
      n_permutations = n_perm
    )
    
    top_features <- explanation %>%
      arrange(desc(abs(feature_weight))) %>%
      pull(feature) %>%
      unique() %>%
      head(5)
    
    label <- paste0("perm_", n_perm, "_seed_", s)
    lime_results[[label]] <- top_features
  }
}

```

```{r}
# Flatten to frequency table
feature_counts <- unlist(lime_results) %>%
  table() %>%
  sort(decreasing = TRUE) %>%
  as.data.frame()
colnames(feature_counts) <- c("feature", "count")

# Plot frequency of appearance
ggplot(feature_counts, aes(x = reorder(feature, count), y = count)) +
  geom_col(fill = "skyblue") +
  coord_flip() +
  labs(
    title = "Frequency of Feature Appearance in Top-5 (LIME)",
    x = "Feature",
    y = "Count (out of 12)"
  ) +
  theme_minimal()

```

# 14. SHAP Stability Over Seeds and n_permutations

```{r}
# Define settings
shap_nsim_list <- c(10, 50, 100, 1000)
shap_seed_list <- c(1, 2, 3)

# Prediction wrapper for SHAP
pfun_rf <- function(object, newdata) {
  predict(object, newdata, type = "prob")[, "yes"]
}

# Store top-5 features per SHAP explanation
shap_results <- list()

for (nsim in shap_nsim_list) {
  for (s in shap_seed_list) {
    set.seed(s)
    shap_vals <- fastshap::explain(
      object = rf_model,
      X = train_X,
      newdata = test_X[3, , drop = FALSE],
      pred_wrapper = pfun_rf,
      nsim = nsim,
      adjust = TRUE
    )
    
    shap_df <- shap_vals %>%
      as.data.frame() %>%
      t() %>%
      as.data.frame() %>%
      tibble::rownames_to_column("feature") %>%
      rename(shap_value = V1) %>%
      arrange(desc(abs(shap_value)))
    
    label <- paste0("nsim_", nsim, "_seed_", s)
    shap_results[[label]] <- head(shap_df$feature, 5)
  }
}
shap_results
```

```{r}
# Count how often each feature appears
shap_feature_counts <- unlist(shap_results) %>%
  table() %>%
  sort(decreasing = TRUE) %>%
  as.data.frame()
colnames(shap_feature_counts) <- c("feature", "count")

ggplot(shap_feature_counts, aes(x = reorder(feature, count), y = count)) +
  geom_col(fill = "blue") +
  coord_flip() +
  labs(
    title = "Frequency of Feature Appearance in Top-5 (SHAP)",
    x = "Feature",
    y = "Count (out of 12)"
  ) +
  theme_minimal()

```
