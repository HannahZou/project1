# Introduction

Machine learning is increasingly being integrated across diverse domains to enhance data-driven decision-making. In many cases, complex black-box models are preferred because they are believed to offer better predictive performance (Bohanec, Robnik-Šikonja, & Kljajić Borštnar, 2017). In contrast, interpretable models are frequently overlooked or undervalued, as they are commonly associated with reduced accuracy, despite their potential to offer transparent and trustworthy insights (Li et al., 2022).

Interpretability helps build user trust, supports regulatory requirements, and makes model results easier to act on (Hong, Hullman, & Bertini, 2020). To make complex models more understandable, post-hoc explanation tools like LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive Explanations) are widely used. These methods help explain predictions from black-box models like Random Forests.

However, a major concern is how reliable these explanation methods are. Both LIME and SHAP use random processes—LIME relies on local sampling and perturbations, while SHAP uses Monte Carlo simulations to estimate Shapley values (Henninger & Strobl, 2023). This means their explanations can vary depending on random seed settings and parameter values (e.g., number of permutations or simulations). For those who are aiming to obtain consistent and reliable model explanations, it becomes essential to evaluate the sensitivity of interpretability methods to variations in factors such as data sampling, model parameters, or random seeds. Such assessments would help to ensure that the insights derived from these explanations remain stable and trustworthy across different conditions.

This study investigates how stable the explanations from LIME and SHAP are when explaining a single prediction made by a Random Forest classifier. We examine whether these methods consistently identify the same top features. Furthermore, we compare these locally identified features with the global feature importance rankings derived directly from the Random Forest model, aiming to understand the alignment between local and global interpretability perspectives.

Using the Bank Marketing dataset from the UCI Machine Learning Repository, we perform a controlled analysis over multiple runs. We test how different settings affect the results and assess the agreement between local and global explanations. These early findings provide a foundation for larger studies involving more predictions, models, or datasets.
